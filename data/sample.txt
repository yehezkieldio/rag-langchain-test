Hierarchical RAG POC Document

Section 1: Introduction to Hybrid Search

Hybrid search combines the strengths of keyword-based search (like BM25 or PostgreSQL FTS using tsvector) and semantic vector search.
Keyword search excels at finding documents with exact term matches, which is crucial for specific names, acronyms, or technical terms.
Vector search, powered by embeddings, captures the semantic meaning and context, allowing it to find relevant documents even if they don't use the exact same keywords.
By combining both, hybrid search often yields more relevant and robust results than either method alone. Techniques like Reciprocal Rank Fusion (RRF) are used to merge the results from both search types effectively.

Section 2: System Architecture

This Proof of Concept uses several components:
- OpenRouter: Provides access to various Large Language Models (LLMs).
- Hugging Face Inference API: Used for generating text embeddings.
- PostgreSQL with pgvector: Stores text chunks and their corresponding vector embeddings for similarity search.
- PostgreSQL Full-Text Search (FTS): Enables keyword-based searching using tsvector and tsquery.
- Redis: Used for caching LLM responses to improve performance and reduce costs.
- LangChain JS: Orchestrates the different components, managing prompts, chains, and interactions.
- Docker: Containerizes the application and its dependencies (Postgres, Redis) for easy deployment and environment consistency.

The core logic involves a router LLM call to classify the user's intent, followed by either a direct conversational response or a RAG process involving hybrid search and final answer generation.

Section 3: Performance Considerations on Low Specs

Running complex AI tasks locally can be challenging on machines with limited CPU, GPU (like the MX110), and RAM.
This POC addresses this by:
- Offloading LLM inference to OpenRouter (cloud-based).
- Offloading embedding generation to Hugging Face (cloud-based).
- Using efficient database indexing (HNSW for vectors, GIN for FTS) in PostgreSQL.
- Implementing caching with Redis.
Containerization with Docker helps manage resources, but the host system's limits still apply. For larger datasets or more complex models, cloud resources would be necessary.